{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxvazquez/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# Athlete ID range (I checked manually)\n",
    "start_id = 1\n",
    "end_id = 16460"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Athlete Information (birthday, country, height, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch athlete data\n",
    "def fetch_athlete_data(athlete_id):\n",
    "    headers = {\n",
    "        'X-Csrf-Token': 'QsiFWuxEY1S9h_-dQgRA_7S5w9uvvmXsjq56QbTPw4i_g_XR68rMCBFFhW6HngBRtHskfN5yjX8GQmawqs8BlQ',\n",
    "        'Referer': 'https://ifsc.results.info',\n",
    "        'Cookie': 'session_id=_verticallife_resultservice_session=6RHN3xZrXnftTiScNfSHg7BVvuebLzGAmC9P5vIpzdySn2vG7VwQpjSZRDHug%2BPKCWlkt831HjLvHsPoVKrzTGsPVR6mqSOtjHB%2Bwht%2Bj39KxYO%2FJlaU6zmh8VhNFEl9bXHiOlPGk8AxnZqiBSYKTxJFCqh34nqdurXfFDcsRnbEtYCixcOdx%2F32E4zYGLVw7DSXXIKOVTUivS43UJZq5zDWPctX95UWm%2FD7%2B6UYT2s0B%2B3XJVPgjMWCMR%2FVZs%2FQC45Gjm4uCpHHe8Yt73nM3J%2Br43V1HuHGSvRpRczrJ4QdovlJHDEpg4rjUA%3D%3D--xlV%2BnBWvX%2BFwNcXI--24wJLAwE%2F8YbvCRvH9nMWQ%3D%3D',  # If session cookies are required\n",
    "    }\n",
    "    url = f\"https://ifsc.results.info/api/v1/athletes/{athlete_id}\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=120)\n",
    "        if response.status_code == 200:\n",
    "            athlete_data = response.json()\n",
    "            return {\n",
    "                'athlete_id': athlete_data['id'],\n",
    "                'firstname': athlete_data['firstname'],\n",
    "                'lastname': athlete_data['lastname'],\n",
    "                'age': athlete_data['age'],\n",
    "                'gender': athlete_data['gender'],\n",
    "                'country': athlete_data['country'],\n",
    "                'height': athlete_data['height'],\n",
    "                'arm_span': athlete_data['arm_span'],\n",
    "                'paraclimbing_sport_class': athlete_data['paraclimbing_sport_class'],\n",
    "                'birthday': athlete_data['birthday'],\n",
    "            }\n",
    "        else:\n",
    "            return None  # Handle non-200 responses gracefully\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for athlete ID {athlete_id}: {e}\")\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for athlete ID 6518: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Error fetching data for athlete ID 6520: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Error fetching data for athlete ID 6516: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Error fetching data for athlete ID 6517: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Error fetching data for athlete ID 6519: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Error fetching data for athlete ID 6521: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Error fetching data for athlete ID 6510: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Error fetching data for athlete ID 6512: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Error fetching data for athlete ID 6523: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Error fetching data for athlete ID 6522: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Error fetching data for athlete ID 6514: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))Error fetching data for athlete ID 6513: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "\n",
      "Error fetching data for athlete ID 6524: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Error fetching data for athlete ID 6515: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Error fetching data for athlete ID 6511: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Retry attempt 1 for 15 failed athlete IDs\n",
      "Scraped 16256 athletes\n"
     ]
    }
   ],
   "source": [
    "def retry_failed_athlete_info(failed_ids, max_retries=2, delay=2):\n",
    "    retry_results = []\n",
    "    for retry_count in range(max_retries):\n",
    "        print(f\"Retry attempt {retry_count + 1} for {len(failed_ids)} failed athlete IDs\")\n",
    "        retry_futures = []\n",
    "        with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            retry_futures = {executor.submit(fetch_athlete_data, athlete_id): athlete_id for athlete_id in failed_ids}\n",
    "            failed_ids = []  # Reset failed_ids list for next retry\n",
    "        \n",
    "            for future in as_completed(retry_futures):\n",
    "                athlete_data = future.result()\n",
    "                athlete_id = retry_futures[future]\n",
    "\n",
    "                if isinstance(athlete_data, Exception): # If an error occured during the fetch\n",
    "                    failed_ids.append(athlete_id)\n",
    "                elif athlete_data:\n",
    "                    retry_results.append(athlete_data)\n",
    "        \n",
    "        if not failed_ids:\n",
    "            break  # Exit loop if no more failed IDs\n",
    "        \n",
    "        time.sleep(delay)  # Wait between retries to avoid overloading the server\n",
    "    \n",
    "    if failed_ids:\n",
    "        print(f\"Final failed athlete IDs after {max_retries} retries: {failed_ids}\")\n",
    "    \n",
    "    return retry_results, failed_ids\n",
    "\n",
    "\n",
    "# Function to scrape athlete data in parallel\n",
    "def scrape_athletes_parallel(start_id, end_id, max_workers=25):\n",
    "    athletes_info = []\n",
    "    failed_ids = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks for each athlete ID\n",
    "        futures = {executor.submit(fetch_athlete_data, athlete_id): athlete_id for athlete_id in range(start_id, end_id + 1)}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            athlete_data = future.result()\n",
    "            athlete_id = futures[future]\n",
    "\n",
    "            if isinstance(athlete_data, Exception):  # If an error occured during the fetch\n",
    "                failed_ids.append(athlete_id)\n",
    "            elif athlete_data:\n",
    "                athletes_info.append(athlete_data)\n",
    "    \n",
    "\n",
    "    # Retry for failed athlete IDs\n",
    "    if failed_ids:\n",
    "        retry_results, failed_ids = retry_failed_athlete_info(failed_ids)\n",
    "        athletes_info.extend(retry_results)  # Add successful retries\n",
    "\n",
    "    return athletes_info, failed_ids\n",
    "\n",
    "\n",
    "\n",
    "# Scrape athlete data from the range and create a DataFrame\n",
    "athletes_info_list, failed_ids = scrape_athletes_parallel(start_id, end_id)\n",
    "athletes_info_df = pd.DataFrame(athletes_info_list)\n",
    "\n",
    "athletes_info_df.to_csv('athlete_information.csv', index=False)\n",
    "\n",
    "print(f\"Scraped {len(athletes_info_df)} athletes\")\n",
    "if failed_ids:\n",
    "    print(f\"Failed to fetch data for {len(failed_ids)} athlete IDs after retries: {failed_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Athlete Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_athlete_results(athlete_id):\n",
    "    headers = {\n",
    "        'X-Csrf-Token': 'QsiFWuxEY1S9h_-dQgRA_7S5w9uvvmXsjq56QbTPw4i_g_XR68rMCBFFhW6HngBRtHskfN5yjX8GQmawqs8BlQ',\n",
    "        'Referer': 'https://ifsc.results.info',\n",
    "        'Cookie': 'session_id=_verticallife_resultservice_session=6RHN3xZrXnftTiScNfSHg7BVvuebLzGAmC9P5vIpzdySn2vG7VwQpjSZRDHug%2BPKCWlkt831HjLvHsPoVKrzTGsPVR6mqSOtjHB%2Bwht%2Bj39KxYO%2FJlaU6zmh8VhNFEl9bXHiOlPGk8AxnZqiBSYKTxJFCqh34nqdurXfFDcsRnbEtYCixcOdx%2F32E4zYGLVw7DSXXIKOVTUivS43UJZq5zDWPctX95UWm%2FD7%2B6UYT2s0B%2B3XJVPgjMWCMR%2FVZs%2FQC45Gjm4uCpHHe8Yt73nM3J%2Br43V1HuHGSvRpRczrJ4QdovlJHDEpg4rjUA%3D%3D--xlV%2BnBWvX%2BFwNcXI--24wJLAwE%2F8YbvCRvH9nMWQ%3D%3D',  # If session cookies are required\n",
    "    }\n",
    "    url = f\"https://ifsc.results.info/api/v1/athletes/{athlete_id}\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            athlete_data = response.json()\n",
    "            # Extract the athlete's results from the 'all_results' field\n",
    "            results = []\n",
    "            for result in athlete_data.get('all_results', []):\n",
    "                results.append({\n",
    "                    'athlete_id': athlete_id,\n",
    "                    'rank': result['rank'],\n",
    "                    'discipline': result['discipline'],\n",
    "                    'season': result['season'],\n",
    "                    'date': result['date'],\n",
    "                    'event_id': result['event_id'],\n",
    "                    'event_location': result['event_location'],\n",
    "                    'd_cat': result['d_cat'],\n",
    "                })\n",
    "            return results\n",
    "        else:\n",
    "            return None  # Handle non-200 responses gracefully\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching results for athlete ID {athlete_id}: {e}\")\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed athlete ID 1000\n",
      "Processed athlete ID 2000\n",
      "Error fetching results for athlete ID 936: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Processed athlete ID 5000\n",
      "Processed athlete ID 6000\n",
      "Processed athlete ID 7000\n",
      "Processed athlete ID 8000\n",
      "Processed athlete ID 9000\n",
      "Processed athlete ID 10000\n",
      "Processed athlete ID 11000\n",
      "Processed athlete ID 12000\n",
      "Processed athlete ID 14000\n",
      "Processed athlete ID 15000\n",
      "Error fetching results for athlete ID 16013: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Retry attempt 1 for 2 failed athlete IDs\n",
      "Scraped results for 135089 athlete events\n"
     ]
    }
   ],
   "source": [
    "def retry_failed_athletes(failed_ids, max_retries=3, delay=2):\n",
    "    retry_results = []\n",
    "    for retry_count in range(max_retries):\n",
    "        print(f\"Retry attempt {retry_count + 1} for {len(failed_ids)} failed athlete IDs\")\n",
    "        retry_futures = []\n",
    "        with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            retry_futures = {executor.submit(fetch_athlete_results, athlete_id): athlete_id for athlete_id in failed_ids}\n",
    "            failed_ids = []  # Reset failed_ids list for next retry\n",
    "        \n",
    "            for future in as_completed(retry_futures):\n",
    "                results = future.result()\n",
    "                athlete_id = retry_futures[future]\n",
    "\n",
    "                if isinstance(results, Exception): # If an error occured during the fetch\n",
    "                    failed_ids.append(athlete_id)\n",
    "                elif results:\n",
    "                    retry_results.extend(results)\n",
    "        \n",
    "        if not failed_ids:\n",
    "            break  # Exit loop if no more failed IDs\n",
    "        \n",
    "        time.sleep(delay)  # Wait between retries to avoid overloading the server\n",
    "    \n",
    "    if failed_ids:\n",
    "        print(f\"Final failed athlete IDs after {max_retries} retries: {failed_ids}\")\n",
    "    \n",
    "    return retry_results, failed_ids\n",
    "\n",
    "\n",
    "\n",
    "def scrape_athlete_results_parallel(start_id, end_id, max_workers=60):\n",
    "    athlete_results = []\n",
    "    failed_ids = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(fetch_athlete_results, athlete_id): athlete_id for athlete_id in range(start_id, end_id + 1)}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            results = future.result()\n",
    "            athlete_id = futures[future]\n",
    "            \n",
    "            if isinstance(results, Exception):  # If an error occured during the fetch\n",
    "                failed_ids.append(athlete_id)\n",
    "            elif results:\n",
    "                athlete_results.extend(results)\n",
    "\n",
    "                if athlete_id % 1000 == 0:\n",
    "                    print(f\"Processed athlete ID {athlete_id}\")\n",
    "                \n",
    "    \n",
    "    # Retry for failed athlete IDs\n",
    "    if failed_ids:\n",
    "        retry_results, failed_ids = retry_failed_athletes(failed_ids)\n",
    "        athlete_results.extend(retry_results)  # Add successful retries\n",
    "    \n",
    "    return athlete_results, failed_ids\n",
    "\n",
    "\n",
    "athlete_results_list, failed_ids = scrape_athlete_results_parallel(start_id, end_id)\n",
    "athlete_results_df = pd.DataFrame(athlete_results_list)\n",
    "\n",
    "athlete_results_df.to_csv('athlete_results.csv', index=False)\n",
    "\n",
    "print(f\"Scraped results for {len(athlete_results_df)} athlete events\")\n",
    "if failed_ids:\n",
    "    print(f\"Failed to fetch data for {len(failed_ids)} athlete IDs after retries: {failed_ids}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
